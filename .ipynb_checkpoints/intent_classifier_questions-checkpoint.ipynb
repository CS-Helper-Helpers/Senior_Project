{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/a-brief-introduction-to-intent-classification-96fda6b1f557\n",
    "# Classifies into 21 intents\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "\n",
    "    print(df.head())\n",
    "    intent = df[\"Intent\"]\n",
    "    unique_intent = list(set(intent))\n",
    "    sentences = list(df[\"Sentence\"])\n",
    "  \n",
    "    return (intent, unique_intent, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Sentence              Intent\n",
      "0                 When do classes start?  faq.important_date\n",
      "1          When does the semester start?  faq.important_date\n",
      "2      What day does the semester start?  faq.important_date\n",
      "3     Which day does the semester start?  faq.important_date\n",
      "4  What date does the semester start on?  faq.important_date\n"
     ]
    }
   ],
   "source": [
    "intent, unique_intent, sentences = load_dataset(\"questions_categories.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get stopwords and punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        # lemmatizing\n",
    "        words.append([i.lower() for i in w])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "[['when', 'do', 'classes', 'start'], ['when', 'does', 'the', 'semester', 'start'], ['what', 'day', 'does', 'the', 'semester', 'start']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer( words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    \"\"\"Create tokenizer\n",
    "    \"\"\"\n",
    "    token = Tokenizer(filters=filters)\n",
    "    token.fit_on_texts(words)\n",
    "    return token\n",
    "\n",
    "def get_max_length(words):\n",
    "    \"\"\"Gets max length of a word\n",
    "    \"\"\"\n",
    "    return(len(max(words, key=len)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size =  158  and Maximum length =  17\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = get_max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab size = \", vocab_size, \" and Maximum length = \", max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen=max_length, padding=\"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  5, 24, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0],\n",
       "       [ 6, 12,  3, 21, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0],\n",
       "       [ 9, 14, 12,  3, 21, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0],\n",
       "       [66, 14, 12,  3, 21, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0],\n",
       "       [ 9, 94, 12,  3, 21, 17, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)\n",
    "padded_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (119, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \", padded_doc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intent, filters='!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class.professor': 1,\n",
       " 'faq.important_date': 2,\n",
       " 'department.time': 3,\n",
       " 'faq.department': 4,\n",
       " 'faq.employee': 5,\n",
       " 'faq.student': 6,\n",
       " 'faq.class': 7,\n",
       " 'class.location': 8,\n",
       " 'faq.professor': 9,\n",
       " 'class.time': 10}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode output given intent and tokenizer and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 1)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one hot encoding\n",
    "Example of one hot encoding:\n",
    "Consider a domain of [ a, e, i, o, u] and an intent of [ a, i, u ]\n",
    "The one hot encoding is [ 1, 0, 1, 0, 1] for the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "    o = OneHotEncoder(sparse=False)\n",
    "    return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 9]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 10)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (95, 17) and train_Y = (95, 10)\n",
      "Shape of val_X = (24, 17) and val_Y = (24, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model\n",
    "[ conv ] -> [ batch norm ] -> [ relu ]\n",
    "\n",
    "A sequential model allows you to create models layer-by-layer in a step-by-step fashion\n",
    "\n",
    "We instantiate the sequential model first, then add each layer one at a time.\n",
    "Layers:\n",
    "    \n",
    "    Embedding:\n",
    "        Vocab size:\n",
    "        Input length:\n",
    "    Bidirectional:\n",
    "        LSTM:\n",
    "    Dense: Relu\n",
    "    Dropout: 0.5\n",
    "    Dense: Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(10, activation = \"softmax\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and give summary of model\n",
    "#### Compile\n",
    "A loss function (or objective function, or optimization score function) is one of the two parameters required to compile a model. We use categorical cross entropy to train a CNN to output a probability over the C classes for each image. It is used for multi-class classification. It is also called softmax loss, a softmax activation plus a cross-entropy loss.\n",
    "\n",
    "Adam is an adaptive learning rate optimization algorithm designed to train deep neural nets. Adam computes individual learning rates for different parameters, using the first and second moments of gradient to adapt the learning rate for each weight of the neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 17, 128)           20224     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21)                693       \n",
      "=================================================================\n",
      "Total params: 292,309\n",
      "Trainable params: 272,085\n",
      "Non-trainable params: 20,224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Uses checkpoint to save best model at each training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (95, 10) was passed for an output of shape (None, 21) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-307-61a7e03eee6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2536\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2538\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2540\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    742\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (95, 10) was passed for an output of shape (None, 21) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "hist = model.fit(train_X, train_Y, epochs = 100, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Loads the best model found from training above, model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predicted probability for a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictions(text):\n",
    "  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "  test_word = word_tokenize(clean)\n",
    "  test_word = [w.lower() for w in test_word]\n",
    "  test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "  print(test_word)\n",
    "  #Check for unknown words\n",
    "  if [] in test_ls:\n",
    "    test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    " \n",
    "  x = padding_doc(test_ls, max_length)\n",
    "  \n",
    "  pred = model.predict_proba(x)\n",
    "  \n",
    "  \n",
    "  return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get final output for the prediction and the classes of intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_final_output(pred, classes):\n",
    "  predictions = pred[0]\n",
    " \n",
    "  classes = np.array(classes)\n",
    "  ids = np.argsort(-predictions)\n",
    "  classes = classes[ids]\n",
    "  predictions = -np.sort(-predictions)\n",
    " \n",
    "  for i in range(pred.shape[1]):\n",
    "    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model =]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'you', 'help', 'me']\n",
      "commonQ.assist has confidence = 0.32932204\n",
      "commonQ.query has confidence = 0.139143\n",
      "contact.contact has confidence = 0.09659231\n",
      "commonQ.bot has confidence = 0.08079043\n",
      "faq.biz_new has confidence = 0.077273935\n",
      "commonQ.name has confidence = 0.07718031\n",
      "commonQ.how has confidence = 0.069322\n",
      "faq.bad_service has confidence = 0.059703138\n",
      "faq.apply_register has confidence = 0.022363504\n",
      "commonQ.wait has confidence = 0.014274337\n",
      "commonQ.not_giving has confidence = 0.013744186\n",
      "faq.aadhaar_missing has confidence = 0.006254018\n",
      "commonQ.just_details has confidence = 0.0044244975\n",
      "faq.application_process has confidence = 0.0040695267\n",
      "faq.borrow_use has confidence = 0.0026477\n",
      "faq.borrow_limit has confidence = 0.0013174948\n",
      "faq.biz_simpler has confidence = 0.00070938957\n",
      "faq.approval_time has confidence = 0.00048008468\n",
      "faq.biz_category_missing has confidence = 0.00029588217\n",
      "faq.banking_option_missing has confidence = 6.383373e-05\n",
      "faq.address_proof has confidence = 2.8412622e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Can you help me?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'do', 'i', 'apply', 'for', 'this', 'position']\n",
      "faq.application_process has confidence = 0.9228019\n",
      "faq.apply_register has confidence = 0.07120995\n",
      "commonQ.assist has confidence = 0.0022965271\n",
      "contact.contact has confidence = 0.0022584863\n",
      "faq.borrow_use has confidence = 0.0011869266\n",
      "faq.biz_simpler has confidence = 7.222695e-05\n",
      "faq.approval_time has confidence = 3.9839113e-05\n",
      "faq.biz_new has confidence = 3.6369358e-05\n",
      "faq.borrow_limit has confidence = 2.3749713e-05\n",
      "faq.bad_service has confidence = 2.2953378e-05\n",
      "faq.biz_category_missing has confidence = 2.0747395e-05\n",
      "faq.banking_option_missing has confidence = 1.7692144e-05\n",
      "commonQ.how has confidence = 8.244553e-06\n",
      "commonQ.wait has confidence = 1.4011844e-06\n",
      "commonQ.query has confidence = 1.3473338e-06\n",
      "commonQ.name has confidence = 1.0343318e-06\n",
      "faq.aadhaar_missing has confidence = 3.699744e-07\n",
      "faq.address_proof has confidence = 1.3429752e-07\n",
      "commonQ.just_details has confidence = 1.1053147e-07\n",
      "commonQ.bot has confidence = 7.3278206e-09\n",
      "commonQ.not_giving has confidence = 4.5010147e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"How do I apply for this position?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'for', 'me']\n",
      "faq.apply_register has confidence = 0.2630215\n",
      "commonQ.assist has confidence = 0.1846362\n",
      "commonQ.how has confidence = 0.119165204\n",
      "commonQ.wait has confidence = 0.112347364\n",
      "contact.contact has confidence = 0.07797397\n",
      "commonQ.name has confidence = 0.052337676\n",
      "commonQ.bot has confidence = 0.045500476\n",
      "commonQ.query has confidence = 0.030452987\n",
      "faq.aadhaar_missing has confidence = 0.027099965\n",
      "faq.biz_new has confidence = 0.01953664\n",
      "commonQ.not_giving has confidence = 0.015911063\n",
      "faq.application_process has confidence = 0.01587559\n",
      "faq.bad_service has confidence = 0.01562931\n",
      "commonQ.just_details has confidence = 0.012118423\n",
      "faq.approval_time has confidence = 0.0032076482\n",
      "faq.borrow_use has confidence = 0.0016377135\n",
      "faq.banking_option_missing has confidence = 0.0010719093\n",
      "faq.biz_category_missing has confidence = 0.00094919227\n",
      "faq.borrow_limit has confidence = 0.0007261337\n",
      "faq.address_proof has confidence = 0.00048034557\n",
      "faq.biz_simpler has confidence = 0.00032065253\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Wait for me\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'can', 'i', 'call', 'to', 'help', 'me']\n",
      "faq.apply_register has confidence = 0.89374787\n",
      "faq.application_process has confidence = 0.036169574\n",
      "commonQ.assist has confidence = 0.031955425\n",
      "contact.contact has confidence = 0.030594548\n",
      "commonQ.how has confidence = 0.005335092\n",
      "commonQ.wait has confidence = 0.001275502\n",
      "commonQ.name has confidence = 0.00029222755\n",
      "faq.bad_service has confidence = 0.00012833613\n",
      "faq.biz_new has confidence = 0.00011115512\n",
      "commonQ.query has confidence = 0.00010855391\n",
      "faq.approval_time has confidence = 8.3946055e-05\n",
      "commonQ.just_details has confidence = 5.583135e-05\n",
      "faq.aadhaar_missing has confidence = 3.8708153e-05\n",
      "faq.banking_option_missing has confidence = 3.30851e-05\n",
      "faq.borrow_use has confidence = 2.912586e-05\n",
      "commonQ.bot has confidence = 1.8081726e-05\n",
      "faq.biz_category_missing has confidence = 7.941922e-06\n",
      "commonQ.not_giving has confidence = 7.613979e-06\n",
      "faq.borrow_limit has confidence = 4.9950563e-06\n",
      "faq.biz_simpler has confidence = 1.3852948e-06\n",
      "faq.address_proof has confidence = 1.0545323e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Who can I call to help me?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
