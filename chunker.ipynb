{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kathrine.swe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Important Dates Slots\n",
    "In this notebook, we take a look at Important Date intents and chunk them to find the necessary slot variables so that we may query our database for a response to our user.\n",
    "\n",
    "### Definitions\n",
    "intents:\n",
    "\n",
    "chunking:\n",
    "\n",
    "slots:\n",
    "\n",
    "## Below we examine some intents about breaks\n",
    "\"When is fall break?\"\n",
    "\n",
    "\"When is spring break?\"\n",
    "\n",
    "\"When is the next break?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('When', 'WRB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('spring', 'VBG'),\n",
       "  ('break', 'NN'),\n",
       "  ('?', '.')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spring_break=\"When is spring break?\"\n",
    "ss=nt.sent_tokenize(spring_break)\n",
    "tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('When', 'WRB'), ('is', 'VBZ'), ('fall', 'DT'), ('break', 'NN'), ('?', '.')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fall_break=\"When is fall break?\"\n",
    "ss=nt.sent_tokenize(fall_break)\n",
    "tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('When', 'WRB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('next', 'JJ'),\n",
       "  ('break', 'NN'),\n",
       "  ('?', '.')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_break=\"When is the next break?\"\n",
    "ss=nt.sent_tokenize(next_break)\n",
    "tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "In the above examples, the POS tagger is not correctly tagging Spring and Fall to be JJs, or adjectives. Rather, 'spring' is tagged as VBG (Verb Gerund) and 'fall' is DT (Determiner).\n",
    "\n",
    "Determiners are words like th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Where', 'WRB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('science', 'NN'),\n",
       "  ('hall', 'NN'),\n",
       "  ('112', 'CD'),\n",
       "  ('?', '.')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Where is science hall 112?\"\n",
    "ss=nt.sent_tokenize(text)\n",
    "tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Where', 'WRB'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('Intro', 'NNP'),\n",
       "  ('to', 'TO'),\n",
       "  ('Philosophy', 'NNP'),\n",
       "  ('?', '.')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Where is Intro to Philosophy?\"\n",
    "ss=nt.sent_tokenize(text)\n",
    "tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_NN(sentence):\n",
    "    grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        # Nouns and Adjectives, terminated with Nouns\n",
    "        {<NN.*>*<NN.*>}\n",
    "\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        # Above, connected with in/of/etc...\n",
    "        {<NBAR><IN><NBAR>}\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    ne = set()\n",
    "    chunk = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "    for tree in chunk.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        ne.add(' '.join([child[0] for child in tree.leaves()]))\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end', 'semester'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_NN(\"When is the end of the semester?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cardinal_digit(sent):\n",
    "    grammar = r\"\"\"\n",
    "    CD:\n",
    "        # Cardinal Digits\n",
    "        {<CD.*>}\n",
    "\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    ne = set()\n",
    "    chunk = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    for tree in chunk.subtrees(filter=lambda t: t.label() == 'CD'):\n",
    "        ne.add(' '.join([child[0] for child in tree.leaves()]))\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'112'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_cardinal_digit(\"Where is room 112?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who teaches CS448?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adjective_noun(sent):\n",
    "    grammar = r\"\"\"\n",
    "    CD:\n",
    "        # Adjective followed by noun\n",
    "        {<JJ><NN>}\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    ne = set()\n",
    "    chunk = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    for tree in chunk.subtrees(filter=lambda t: t.label() == 'CD'):\n",
    "        ne.add(' '.join([child[0] for child in tree.leaves()]))\n",
    "    return ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next break'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_adjective_noun(\"When is the next break?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_adjective_noun(sent):\n",
    "    grammar = r\"\"\"\n",
    "    CD:\n",
    "        # Adjective followed by noun\n",
    "        {<JJ><NN>}\n",
    "    \"\"\"\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    ne = set()\n",
    "    chunk = chunker.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
    "    for tree in chunk.subtrees(filter=lambda t: t.label() == 'CD'):\n",
    "        ne.add(' '.join([child[0] for child in tree.leaves()]))\n",
    "    return ne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
